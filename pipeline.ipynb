{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4a99f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline start: 2025-10-16T21:08:16\n",
      "RAW_DIR=c:\\Users\\juanl\\Downloads\\final\\csv\\Daft17_Group01_PF\\data_raw | OUT_DIR=c:\\Users\\juanl\\Downloads\\final\\csv\\Daft17_Group01_PF\\data_final\n",
      "Stage 1: Load and clean all *_raw files\n",
      "Loaded and cleaned common_player_info_raw.csv (4171 rows, 33 cols)\n",
      "Loaded and cleaned game_raw.csv (65698 rows, 55 cols)\n",
      "Loaded and cleaned game_summary_raw.csv (58110 rows, 14 cols)\n",
      "Loaded and cleaned other_stats_raw.csv (28271 rows, 26 cols)\n",
      "Loaded and cleaned player_raw.csv (4831 rows, 5 cols)\n",
      "Loaded and cleaned team_raw.csv (30 rows, 7 cols)\n",
      "Normalizing ID columns\n",
      "common_player_info: renamed {'person_id': 'player_id', 'team_id': 'team_id'}\n",
      "game: renamed {'game_id': 'game_id'}\n",
      "game_summary: renamed {'game_id': 'game_id'}\n",
      "other_stats: renamed {'game_id': 'game_id'}\n",
      "player: renamed {'id': 'player_id'}\n",
      "team: renamed {'id': 'team_id'}\n",
      "Normalized common_player_info (height, weight, season_exp)\n",
      "Stage 2: PK/FK validation\n",
      "PK team_id: True. Duplicate rows: 0\n",
      "PK player_id: True. Duplicate rows: 0\n",
      "PK game_id: False. Duplicate rows: 56\n",
      "PK common_player_info.player_id: True. Duplicate rows: 0\n",
      "Stage 3: Export final tables\n",
      "Saved common_player_info_final.csv (4171 rows, 33 cols)\n",
      "Saved game_final.csv (65698 rows, 55 cols)\n",
      "Saved game_summary_final.csv (58110 rows, 14 cols)\n",
      "Saved other_stats_final.csv (28271 rows, 26 cols)\n",
      "Saved player_final.csv (4831 rows, 5 cols)\n",
      "Saved team_final.csv (30 rows, 7 cols)\n",
      "No ingestion changes since last run.\n",
      "Saved ingestion snapshot to data_final/data_status.csv\n",
      "Pipeline completed in 8.87s\n"
     ]
    }
   ],
   "source": [
    "# PIPELINE FINAL (ajustado a las reglas exactas de normalización para common_player_info)\n",
    "# - Lee *_raw.* desde ./data_raw\n",
    "# - Limpieza general robusta\n",
    "# - En common_player_info:\n",
    "#       * height: reemplaza \"-\" por \",\"\n",
    "#       * weight: si tiene un 0 de más (como 900 en vez de 90), lo corrige\n",
    "#       * season_exp: mantiene ceros tal como están\n",
    "# - No crea columnas nuevas, no cambia unidades\n",
    "# - Mantiene PK/FK, logs y snapshot de ingesta\n",
    "\n",
    "import os, re, time\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ===== CONFIG =====\n",
    "start_ts = datetime.now()\n",
    "CWD = os.getcwd()\n",
    "RAW_DIR = os.path.join(CWD, \"data_raw\")\n",
    "OUT_DIR = os.path.join(CWD, \"data_final\")\n",
    "os.makedirs(RAW_DIR, exist_ok=True)\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "LOG_PATH = os.path.join(OUT_DIR, f\"pipeline_run_{start_ts.strftime('%Y%m%d_%H%M%S')}.txt\")\n",
    "\n",
    "def log(msg):\n",
    "    print(msg)\n",
    "    with open(LOG_PATH, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(str(msg) + \"\\n\")\n",
    "\n",
    "log(f\"Pipeline start: {start_ts.isoformat(timespec='seconds')}\")\n",
    "log(f\"RAW_DIR={RAW_DIR} | OUT_DIR={OUT_DIR}\")\n",
    "\n",
    "# ===== HELPERS =====\n",
    "def snake_case(name: str) -> str:\n",
    "    name = re.sub(r\"[^\\w]+\", \"_\", name)\n",
    "    name = re.sub(r\"([a-z0-9])([A-Z])\", r\"\\1_\\2\", name)\n",
    "    name = re.sub(r\"_+\", \"_\", name)\n",
    "    return name.lower().strip(\"_\")\n",
    "\n",
    "def load_any(path: str) -> pd.DataFrame:\n",
    "    ext = os.path.splitext(path)[1].lower()\n",
    "    if ext == \".csv\": return pd.read_csv(path, low_memory=False)\n",
    "    if ext in [\".xlsx\", \".xls\"]: return pd.read_excel(path)\n",
    "    if ext == \".json\":\n",
    "        try: return pd.read_json(path, orient=\"records\")\n",
    "        except ValueError: return pd.read_json(path, lines=True)\n",
    "    if ext == \".parquet\": return pd.read_parquet(path)\n",
    "    raise ValueError(f\"Unsupported file type: {ext}\")\n",
    "\n",
    "def clean_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Limpieza segura y básica\"\"\"\n",
    "    df = df.copy()\n",
    "    df.columns = [snake_case(c) for c in df.columns]\n",
    "    df = df.dropna(how=\"all\")\n",
    "    for c in df.columns:\n",
    "        try:\n",
    "            df[c] = df[c].astype(str).str.strip().replace(\n",
    "                {\"\": np.nan, \"None\": np.nan, \"NULL\": np.nan, \"nan\": np.nan, \"NaN\": np.nan}\n",
    "            )\n",
    "        except Exception:\n",
    "            pass\n",
    "        c_low = c.lower()\n",
    "        if any(k in c_low for k in [\"id\", \"game_id\", \"player_id\", \"person_id\", \"team_id\"]):\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\").astype(\"Int64\")\n",
    "        elif any(k in c_low for k in [\"date\", \"dt\", \"fecha\"]):\n",
    "            df[c] = pd.to_datetime(df[c], errors=\"coerce\")\n",
    "    return df\n",
    "\n",
    "# ===== NORMALIZADOR DE common_player_info =====\n",
    "def normalize_common_player_info(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Aplica solo las transformaciones solicitadas\"\"\"\n",
    "    if df.empty:\n",
    "        return df\n",
    "    df = df.copy()\n",
    "\n",
    "    # Renombres típicos de ID\n",
    "    cols = {c.lower(): c for c in df.columns}\n",
    "    if \"person_id\" in cols: df.rename(columns={cols[\"person_id\"]: \"player_id\"}, inplace=True)\n",
    "    if \"playerid\" in cols: df.rename(columns={cols[\"playerid\"]: \"player_id\"}, inplace=True)\n",
    "    if \"teamid\" in cols: df.rename(columns={cols[\"teamid\"]: \"team_id\"}, inplace=True)\n",
    "\n",
    "    # --- height: solo reemplaza \"-\" por \",\"\n",
    "    if \"height\" in df.columns:\n",
    "        df[\"height\"] = df[\"height\"].astype(str).str.replace(\"-\", \",\", regex=False)\n",
    "\n",
    "    # --- weight: si pesa más de 500, dividir por 10\n",
    "    if \"weight\" in df.columns:\n",
    "        df[\"weight\"] = pd.to_numeric(df[\"weight\"], errors=\"coerce\")\n",
    "        df[\"weight\"] = df[\"weight\"].apply(\n",
    "            lambda x: x / 10 if pd.notna(x) and x > 500 else x\n",
    "    )\n",
    "\n",
    "\n",
    "    # --- season_exp: solo aseguramos que sea numérico, mantenemos ceros\n",
    "    if \"season_exp\" in df.columns:\n",
    "        df[\"season_exp\"] = df[\"season_exp\"].replace({\"R\": \"0\"})\n",
    "        df[\"season_exp\"] = pd.to_numeric(df[\"season_exp\"], errors=\"coerce\").fillna(0).astype(\"Int64\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# ===== VALIDACIONES =====\n",
    "def validate_pk(df, cols):\n",
    "    if not all(c in df.columns for c in cols):\n",
    "        return False, f\"Missing columns: {set(cols)-set(df.columns)}\"\n",
    "    dups = df.duplicated(subset=cols).sum()\n",
    "    return dups == 0, f\"Duplicate rows: {dups}\"\n",
    "\n",
    "def validate_fk(df_from, col_from, df_to, col_to):\n",
    "    if col_from not in df_from.columns or col_to not in df_to.columns:\n",
    "        return False, f\"Columns not found: {col_from} or {col_to}\"\n",
    "    missing = ~df_from[col_from].isin(df_to[col_to])\n",
    "    return missing.sum() == 0, f\"Unmatched rows: {missing.sum()}\"\n",
    "\n",
    "# ===== STAGE 1: LOAD + CLEAN =====\n",
    "t0 = time.time()\n",
    "log(\"Stage 1: Load and clean all *_raw files\")\n",
    "raw_files = [f for f in os.listdir(RAW_DIR)\n",
    "             if f.endswith((\"_raw.csv\", \"_raw.xlsx\", \"_raw.xls\", \"_raw.json\", \"_raw.parquet\"))]\n",
    "dfs = {}\n",
    "if not raw_files:\n",
    "    log(\"No *_raw files found in data_raw/\")\n",
    "else:\n",
    "    for fname in raw_files:\n",
    "        try:\n",
    "            path = os.path.join(RAW_DIR, fname)\n",
    "            df = load_any(path)\n",
    "            df = clean_dataframe(df)\n",
    "            key = fname.replace(\"_raw\", \"\").split(\".\")[0]\n",
    "            dfs[key] = df\n",
    "            log(f\"Loaded and cleaned {fname} ({df.shape[0]} rows, {df.shape[1]} cols)\")\n",
    "        except Exception as e:\n",
    "            log(f\"Error reading {fname}: {e}\")\n",
    "\n",
    "# ===== NORMALIZAR IDs =====\n",
    "log(\"Normalizing ID columns\")\n",
    "for k, df in dfs.items():\n",
    "    rename_map = {}\n",
    "    for c in df.columns:\n",
    "        lc = c.lower()\n",
    "        if lc in [\"person_id\", \"playerid\", \"player_id\"]: rename_map[c] = \"player_id\"\n",
    "        elif lc in [\"teamid\", \"team_id\"] or (lc == \"id\" and \"team\" in k): rename_map[c] = \"team_id\"\n",
    "        elif lc in [\"gameid\", \"game_id\"]: rename_map[c] = \"game_id\"\n",
    "        elif lc == \"id\" and \"player\" in k: rename_map[c] = \"player_id\"\n",
    "    if rename_map:\n",
    "        df.rename(columns=rename_map, inplace=True)\n",
    "        log(f\"{k}: renamed {rename_map}\")\n",
    "    dfs[k] = df\n",
    "\n",
    "# ===== NORMALIZAR common_player_info =====\n",
    "if \"common_player_info\" in dfs:\n",
    "    dfs[\"common_player_info\"] = normalize_common_player_info(dfs[\"common_player_info\"])\n",
    "    log(\"Normalized common_player_info (height, weight, season_exp)\")\n",
    "\n",
    "# ===== VALIDACIONES =====\n",
    "t1 = time.time()\n",
    "log(\"Stage 2: PK/FK validation\")\n",
    "\n",
    "player = dfs.get(\"player\", pd.DataFrame())\n",
    "team   = dfs.get(\"team\", pd.DataFrame())\n",
    "game   = dfs.get(\"game\", pd.DataFrame())\n",
    "summary= dfs.get(\"game_summary\", pd.DataFrame())\n",
    "stats  = dfs.get(\"other_stats\", pd.DataFrame())\n",
    "cpi    = dfs.get(\"common_player_info\", pd.DataFrame())\n",
    "\n",
    "if not team.empty and \"team_id\" in team.columns:\n",
    "    ok, msg = validate_pk(team, [\"team_id\"]); log(f\"PK team_id: {ok}. {msg}\")\n",
    "if not player.empty and \"player_id\" in player.columns:\n",
    "    ok, msg = validate_pk(player, [\"player_id\"]); log(f\"PK player_id: {ok}. {msg}\")\n",
    "if not game.empty and \"game_id\" in game.columns:\n",
    "    ok, msg = validate_pk(game, [\"game_id\"]); log(f\"PK game_id: {ok}. {msg}\")\n",
    "if not cpi.empty and \"player_id\" in cpi.columns:\n",
    "    ok, msg = validate_pk(cpi, [\"player_id\"]); log(f\"PK common_player_info.player_id: {ok}. {msg}\")\n",
    "\n",
    "# ===== EXPORT =====\n",
    "t2 = time.time()\n",
    "log(\"Stage 3: Export final tables\")\n",
    "\n",
    "for key, df in dfs.items():\n",
    "    out_name = f\"{key}_final.csv\"\n",
    "    out_path = os.path.join(OUT_DIR, out_name)\n",
    "    df.to_csv(out_path, index=False, encoding=\"utf-8-sig\")\n",
    "    log(f\"Saved {out_name} ({df.shape[0]} rows, {df.shape[1]} cols)\")\n",
    "\n",
    "# ===== SNAPSHOT =====\n",
    "status_file = os.path.join(OUT_DIR, \"data_status.csv\")\n",
    "current = {k: (v.shape[0] if isinstance(v, pd.DataFrame) and not v.empty else 0) for k, v in dfs.items()}\n",
    "prev = {}\n",
    "if os.path.exists(status_file):\n",
    "    try:\n",
    "        prev_df = pd.read_csv(status_file)\n",
    "        prev = dict(zip(prev_df[\"table\"], prev_df[\"rows\"]))\n",
    "    except Exception:\n",
    "        prev = {}\n",
    "changes = {k: n - prev.get(k, 0) for k, n in current.items() if n - prev.get(k, 0) != 0}\n",
    "if changes:\n",
    "    log(\"Changes since last run:\")\n",
    "    for k, d in changes.items():\n",
    "        sign = \"+\" if d >= 0 else \"\"\n",
    "        log(f\" - {k}: {sign}{d} rows\")\n",
    "else:\n",
    "    log(\"No ingestion changes since last run.\")\n",
    "pd.DataFrame({\"table\": list(current.keys()), \"rows\": list(current.values())}).to_csv(status_file, index=False, encoding=\"utf-8-sig\")\n",
    "log(f\"Saved ingestion snapshot to data_final/data_status.csv\")\n",
    "\n",
    "# ===== END =====\n",
    "end_ts = datetime.now()\n",
    "log(f\"Pipeline completed in {(end_ts - start_ts).total_seconds():.2f}s\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

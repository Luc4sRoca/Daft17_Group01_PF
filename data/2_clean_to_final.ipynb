{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start: CLEAN -> FINAL (joins first, then advanced cleaning)\n",
      "Loaded player_clean.csv with shape (4831, 5)\n",
      "Loaded team_clean.csv with shape (30, 7)\n",
      "Loaded game_clean.csv with shape (65698, 55)\n",
      "Loaded game_summary_clean.csv with shape (58110, 14)\n",
      "Loaded other_stats_clean.csv with shape (28271, 26)\n",
      "FK stats.game_id -> game.game_id: True. Unmatched rows: 0\n",
      "Coverage summary.game_id in game.game_id: True. Unmatched rows: 0\n",
      "Starting deduplication, mapping and normalization for other_stats...\n",
      "Removed 10 duplicate rows by game_id from other_stats\n",
      "No team_xref.csv found, skipping team ID mapping step.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\juanl\\AppData\\Local\\Temp\\ipykernel_18744\\3174962053.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[c] = pd.to_numeric(df[c], errors=\"coerce\").astype(\"Int64\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fact_team_game_final.csv created with 56522 rows, 16 cols\n",
      "Wrote team_final.csv with shape (30, 7)\n",
      "Wrote player_final.csv with shape (4831, 5)\n",
      "Wrote game_final.csv with shape (65642, 55)\n",
      "Wrote game_summary_final.csv with shape (58021, 14)\n",
      "Wrote other_stats_final.csv with shape (28261, 26)\n",
      "Primary keys prepared: dim_team(team_id), dim_player(player_id), fact_game(game_id), fact_team_game(game_id, team_id)\n",
      "Foreign keys validated: game.home_team_id -> team.team_id, game.visitor_team_id -> team.team_id, stats.game_id -> game.game_id, stats.player_id -> player.player_id\n",
      "Completed CLEAN -> FINAL in 4.20 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os, time, pandas as pd, numpy as np\n",
    "\n",
    "def to_int(df, cols):\n",
    "    for c in cols:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\").astype(\"Int64\")\n",
    "    return df\n",
    "\n",
    "def to_datetime(df, cols):\n",
    "    for c in cols:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_datetime(df[c], errors=\"coerce\")\n",
    "    return df\n",
    "\n",
    "def drop_dups(df, subset):\n",
    "    if all(c in df.columns for c in subset):\n",
    "        return df.drop_duplicates(subset=subset)\n",
    "    return df.drop_duplicates()\n",
    "\n",
    "def validate_fk(df_from, col_from, df_to, col_to):\n",
    "    if col_from not in df_from.columns or col_to not in df_to.columns:\n",
    "        return False, f\"Columns not found: {col_from} or {col_to}\"\n",
    "    missing = ~df_from[col_from].isin(df_to[col_to])\n",
    "    return missing.sum() == 0, f\"Unmatched rows: {missing.sum()}\"\n",
    "\n",
    "start = time.time()\n",
    "print(\"Start: CLEAN -> FINAL (joins first, then advanced cleaning)\")\n",
    "\n",
    "cwd = os.getcwd()\n",
    "files = {\n",
    "    \"player\": \"player_clean.csv\",\n",
    "    \"team\": \"team_clean.csv\",\n",
    "    \"game\": \"game_clean.csv\",\n",
    "    \"summary\": \"game_summary_clean.csv\",\n",
    "    \"stats\": \"other_stats_clean.csv\"\n",
    "}\n",
    "\n",
    "dfs = {}\n",
    "for k, fname in files.items():\n",
    "    p = os.path.join(cwd, fname)\n",
    "    if os.path.exists(p):\n",
    "        dfs[k] = pd.read_csv(p, low_memory=False)\n",
    "        print(f\"Loaded {fname} with shape {dfs[k].shape}\")\n",
    "    else:\n",
    "        print(f\"Warning: {fname} not found\")\n",
    "\n",
    "player = dfs.get(\"player\", pd.DataFrame())\n",
    "team = dfs.get(\"team\", pd.DataFrame())\n",
    "game = dfs.get(\"game\", pd.DataFrame())\n",
    "summary = dfs.get(\"summary\", pd.DataFrame())\n",
    "stats = dfs.get(\"stats\", pd.DataFrame())\n",
    "\n",
    "# Ensure key types for joining\n",
    "for df, ids in [(player, [\"player_id\"]), (team, [\"team_id\"]), (game, [\"game_id\",\"home_team_id\",\"visitor_team_id\",\"season\"]), (summary, [\"game_id\",\"season\"]), (stats, [\"game_id\",\"player_id\",\"team_id\",\"season\"])]:\n",
    "    if not df.empty:\n",
    "        to_int(df, [c for c in ids if c in df.columns])\n",
    "        to_datetime(df, [c for c in df.columns if \"date\" in c])\n",
    "\n",
    "# Stage 1: Joins and FK checks\n",
    "if not game.empty and not team.empty and \"team_id\" in team.columns:\n",
    "    for col in [\"home_team_id\",\"visitor_team_id\"]:\n",
    "        if col in game.columns:\n",
    "            ok, msg = validate_fk(game, col, team, \"team_id\")\n",
    "            print(f\"FK {col} -> team.team_id: {ok}. {msg}\")\n",
    "\n",
    "if not stats.empty and not player.empty and \"player_id\" in player.columns:\n",
    "    ok_p, msg_p = validate_fk(stats, \"player_id\", player, \"player_id\")\n",
    "    print(f\"FK stats.player_id -> player.player_id: {ok_p}. {msg_p}\")\n",
    "if not stats.empty and not game.empty and \"game_id\" in game.columns:\n",
    "    ok_g, msg_g = validate_fk(stats, \"game_id\", game, \"game_id\")\n",
    "    print(f\"FK stats.game_id -> game.game_id: {ok_g}. {msg_g}\")\n",
    "\n",
    "if not summary.empty and not game.empty and \"game_id\" in game.columns:\n",
    "    ok_s, msg_s = validate_fk(summary, \"game_id\", game, \"game_id\")\n",
    "    print(f\"Coverage summary.game_id in game.game_id: {ok_s}. {msg_s}\")\n",
    "\n",
    "# Stage 2: Advanced cleaning\n",
    "if not team.empty:\n",
    "    team = drop_dups(team, [\"team_id\"])\n",
    "if not player.empty:\n",
    "    player = drop_dups(player, [\"player_id\"])\n",
    "if not game.empty:\n",
    "    game = drop_dups(game, [\"game_id\"])\n",
    "if not stats.empty and all(c in stats.columns for c in [\"game_id\",\"player_id\"]):\n",
    "    stats = drop_dups(stats, [\"game_id\",\"player_id\"])\n",
    "if not summary.empty:\n",
    "    summary = drop_dups(summary, [\"game_id\"])\n",
    "\n",
    "if not game.empty and \"home_team_id\" in game.columns and \"visitor_team_id\" in game.columns:\n",
    "    bad = game[game[\"home_team_id\"] == game[\"visitor_team_id\"]]\n",
    "    if not bad.empty:\n",
    "        print(f\"Dropping {len(bad)} rows where home_team_id == visitor_team_id\")\n",
    "        game = game[game[\"home_team_id\"] != game[\"visitor_team_id\"]]\n",
    "\n",
    "to_int(game, [\"game_id\",\"home_team_id\",\"visitor_team_id\",\"season\"])\n",
    "to_int(team, [\"team_id\"])\n",
    "to_int(player, [\"player_id\"])\n",
    "to_int(stats, [\"game_id\",\"player_id\",\"team_id\",\"season\"])\n",
    "\n",
    "# ============================================================\n",
    "# EXTRA: Correcciones y normalizaciÃ³n (corregida)\n",
    "# ============================================================\n",
    "print(\"Starting deduplication, mapping and normalization for other_stats...\")\n",
    "\n",
    "# 1. Deduplicar other_stats por game_id\n",
    "if not stats.empty and \"game_id\" in stats.columns:\n",
    "    before = len(stats)\n",
    "    stats = stats.drop_duplicates(subset=[\"game_id\"], keep=\"first\")\n",
    "    after = len(stats)\n",
    "    print(f\"Removed {before - after} duplicate rows by game_id from other_stats\")\n",
    "\n",
    "# 2. Mapear team IDs si existe un archivo team_xref.csv\n",
    "xref_path = os.path.join(cwd, \"team_xref.csv\")\n",
    "if os.path.exists(xref_path):\n",
    "    xref = pd.read_csv(xref_path)\n",
    "    print(f\"Loaded team_xref.csv with {xref.shape[0]} mappings\")\n",
    "    mapping = dict(zip(xref[\"id_source\"], xref[\"id_canonical\"]))\n",
    "    for df_name, df in [(\"game\", game), (\"stats\", stats)]:\n",
    "        for col in [\"team_id_home\", \"team_id_away\"]:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].replace(mapping)\n",
    "                print(f\"Applied team ID mapping to {df_name}.{col}\")\n",
    "else:\n",
    "    print(\"No team_xref.csv found, skipping team ID mapping step.\")\n",
    "\n",
    "# 3. Normalizar other_stats a nivel equipo-partido (corregido)\n",
    "home_cols = [c for c in stats.columns if c.endswith(\"_home\")]\n",
    "away_cols = [c for c in stats.columns if c.endswith(\"_away\")]\n",
    "base_cols = [c for c in stats.columns if not (c.endswith(\"_home\") or c.endswith(\"_away\"))]\n",
    "\n",
    "if home_cols and away_cols:\n",
    "    df_home = stats[base_cols + home_cols].copy()\n",
    "    df_home.columns = base_cols + [c.replace(\"_home\", \"\") for c in home_cols]\n",
    "    df_home[\"venue\"] = \"home\"\n",
    "    if \"team_id\" not in df_home.columns and \"team_id_home\" in stats.columns:\n",
    "        df_home[\"team_id\"] = stats[\"team_id_home\"].values\n",
    "\n",
    "    df_away = stats[base_cols + away_cols].copy()\n",
    "    df_away.columns = base_cols + [c.replace(\"_away\", \"\") for c in away_cols]\n",
    "    df_away[\"venue\"] = \"away\"\n",
    "    if \"team_id\" not in df_away.columns and \"team_id_away\" in stats.columns:\n",
    "        df_away[\"team_id\"] = stats[\"team_id_away\"].values\n",
    "\n",
    "    fact_team_game = pd.concat([df_home, df_away], ignore_index=True)\n",
    "    assert \"game_id\" in fact_team_game.columns, \"game_id missing after normalization\"\n",
    "    assert \"team_id\" in fact_team_game.columns, \"team_id missing after normalization\"\n",
    "    fact_team_game = fact_team_game.dropna(subset=[\"team_id\",\"game_id\"])\n",
    "    fact_team_game[\"team_id\"] = pd.to_numeric(fact_team_game[\"team_id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    fact_team_game[\"game_id\"] = pd.to_numeric(fact_team_game[\"game_id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "    out_team_game = os.path.join(cwd, \"fact_team_game_final.csv\")\n",
    "    fact_team_game.to_csv(out_team_game, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"fact_team_game_final.csv created with {fact_team_game.shape[0]} rows, {fact_team_game.shape[1]} cols\")\n",
    "else:\n",
    "    print(\"No *_home / *_away columns detected, skipping normalization.\")\n",
    "\n",
    "# ============================================================\n",
    "# Export final tables\n",
    "# ============================================================\n",
    "outputs = {\n",
    "    \"team_final.csv\": team,\n",
    "    \"player_final.csv\": player,\n",
    "    \"game_final.csv\": game,\n",
    "    \"game_summary_final.csv\": summary,\n",
    "    \"other_stats_final.csv\": stats\n",
    "}\n",
    "for fname, df in outputs.items():\n",
    "    outp = os.path.join(cwd, fname)\n",
    "    df.to_csv(outp, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"Wrote {fname} with shape {df.shape}\")\n",
    "\n",
    "print(\"Primary keys prepared: dim_team(team_id), dim_player(player_id), fact_game(game_id), fact_team_game(game_id, team_id)\")\n",
    "print(\"Foreign keys validated: game.home_team_id -> team.team_id, game.visitor_team_id -> team.team_id, stats.game_id -> game.game_id, stats.player_id -> player.player_id\")\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Completed CLEAN -> FINAL in {end - start:.2f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
